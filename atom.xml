<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/xslt/atom.xslt" ?>
<?xml-stylesheet type="text/css" href="/assets/css/atom.css" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>https://yobadia.com/</id>
	<title>Data thoughts</title>
	<updated>2020-08-07T17:09:42+00:00</updated>

	<subtitle>A blog about data, artificial intelligence, society and &lt;br&gt; other interesting topics</subtitle>

	
		
		<author>
			
				<name>Yohan Obadia</name>
			
			
				<email>yohan.obadia@gmail.com</email>
			
			
				<uri>https://yobadia.com</uri>
			
		</author>
	

	<link href="https://yobadia.com/atom.xml" rel="self" type="application/rss+xml" />
	<link href="https://yobadia.com/" rel="alternate" type="text/html" />

	<generator uri="http://jekyllrb.com" version="4.0.0">Jekyll</generator>

	
		<entry>
			<id>https://yobadia.com/probabilities/monty_hall/</id>
			<title>Monty Hall Monte Carlo</title>
			<link href="https://yobadia.com/probabilities/monty_hall/" rel="alternate" type="text/html" title="Monty Hall Monte Carlo" />
			<updated>2020-08-07T00:00:00+00:00</updated>

			
				
				<author>
					
						<name>yo</name>
					
					
					
				</author>
			
			<summary>Fun fact if you ever play that game</summary>
			<content type="html" xml:base="https://yobadia.com/probabilities/monty_hall/">&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\DeclareMathOperator{\EX}{\mathbb{E}}% expected value\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a hypothetical game popularized by Marilyn vos Savant as follow :&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Suppose you&amp;#8217;re on a game show, and you&amp;#8217;re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what&amp;#8217;s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, &quot;Do you want to pick door No. 2?&quot; Is it to your advantage to switch your choice?&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The answer to this probabilty puzzle can be disconcerting. One might expect the probability to be \(\frac{1}{2}\) for both of the remaining doors but in reality the previously selected door carries a probability of hiding a car of \(\frac{1}{3}\) while the other remaining door has a probability of \(\frac{2}{3}\). The conclusion is that you should always switch after the host opens one of the door with a goat !&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To get the rigorous demonstration, I refer you to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Monty_Hall_problem&quot;&gt;Wikipedia&amp;#8217;s page&lt;/a&gt;. Here I will take a different approach by simulating the game repeatedly to estimate the true probability. This method is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot;&gt;Monte Carlo simulation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We first import &lt;strong&gt;numpy&lt;/strong&gt; that will be our only dependency :&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;import&lt;/span&gt; &lt;span style=&quot;color:#B44;font-weight:bold&quot;&gt;numpy&lt;/span&gt; &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;as&lt;/span&gt; np&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s now present an implementation of the game that is meant for readability and allows to run the game once.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;def&lt;/span&gt; &lt;span style=&quot;color:#06B;font-weight:bold&quot;&gt;montyhall_sim&lt;/span&gt;(change_door=&lt;span style=&quot;color:#069&quot;&gt;False&lt;/span&gt;):
    &lt;span style=&quot;color:#D42&quot;&gt;&lt;span style=&quot;color:black&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span&gt;
&lt;/span&gt;&lt;span&gt;    This algorithm could be greatly optimized for performance.&lt;/span&gt;&lt;span&gt;
&lt;/span&gt;&lt;span&gt;    Here it has instead been optimized for understandability for an educational purpose :)&lt;/span&gt;&lt;span&gt;
&lt;/span&gt;&lt;span&gt;    &lt;/span&gt;&lt;span style=&quot;color:black&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;/span&gt;
    &lt;span style=&quot;color:#777&quot;&gt;# Initialize the doors&lt;/span&gt;
    nb_doors = &lt;span style=&quot;color:#00D&quot;&gt;3&lt;/span&gt;
    doors = np.zeros(nb_doors)
    car_door = np.random.randint(nb_doors)
    doors[car_door] = &lt;span style=&quot;color:#00D&quot;&gt;1&lt;/span&gt;
    closed_doors = np.arange(nb_doors)

    &lt;span style=&quot;color:#777&quot;&gt;# First step : the player select a door randomly&lt;/span&gt;
    selected_door = np.random.randint(nb_doors)

    &lt;span style=&quot;color:#777&quot;&gt;# Second step : the host pick randomly among the remaining empty doors and opens it&lt;/span&gt;
    empty_doors = np.argwhere(doors==&lt;span style=&quot;color:#00D&quot;&gt;0&lt;/span&gt;)
    opened_door = np.random.choice(empty_doors[empty_doors!=selected_door])
    closed_doors = np.delete(closed_doors, opened_door)

    &lt;span style=&quot;color:#777&quot;&gt;# Third step : the player either keep his door or switches&lt;/span&gt;
    &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;if&lt;/span&gt; change_door:
        selected_door = np.delete(closed_doors, selected_door)[&lt;span style=&quot;color:#00D&quot;&gt;0&lt;/span&gt;]

    &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;return&lt;/span&gt; doors[selected_door]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The result bellow confirms with empirical data what the theory says about wining probabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;%%time
np.mean([montyhall_sim(&lt;span style=&quot;color:#069&quot;&gt;False&lt;/span&gt;) &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;for&lt;/span&gt; x &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;in&lt;/span&gt; &lt;span style=&quot;color:#369;font-weight:bold&quot;&gt;range&lt;/span&gt;(&lt;span style=&quot;color:#00D&quot;&gt;1000000&lt;/span&gt;)])

&amp;gt; Wall time: &lt;span style=&quot;color:#60E&quot;&gt;28.8&lt;/span&gt; s

&amp;gt; &lt;span style=&quot;color:#60E&quot;&gt;0.333223&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now lets try to improve the simulation speed by reframing the algorithm to learn a bit more about numpy and vectorization. Below we introduce a new implementation that runs all the simulations at once instead of runing them sequentially :&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;def&lt;/span&gt; &lt;span style=&quot;color:#06B;font-weight:bold&quot;&gt;montyhall_sim_optim&lt;/span&gt;(nb_exp, change_door=&lt;span style=&quot;color:#069&quot;&gt;False&lt;/span&gt;):
    &lt;span style=&quot;color:#D42&quot;&gt;&lt;span style=&quot;color:black&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span&gt;
&lt;/span&gt;&lt;span&gt;    A vectorized version leveraging numpy features that returns the average wining score.&lt;/span&gt;&lt;span&gt;
&lt;/span&gt;&lt;span&gt;    &lt;/span&gt;&lt;span style=&quot;color:black&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;/span&gt;

    &lt;span style=&quot;color:#777&quot;&gt;# Initialize the doors&lt;/span&gt;
    rows = np.arange(nb_exp)
    exp = np.zeros((nb_exp,&lt;span style=&quot;color:#00D&quot;&gt;3&lt;/span&gt;))
    exp[rows,np.random.randint(&lt;span style=&quot;color:#00D&quot;&gt;3&lt;/span&gt;,size=nb_exp)] = &lt;span style=&quot;color:#00D&quot;&gt;1&lt;/span&gt;
    car_doors = np.argsort(exp, axis=&lt;span style=&quot;color:#00D&quot;&gt;1&lt;/span&gt;)[:,-&lt;span style=&quot;color:#00D&quot;&gt;1&lt;/span&gt;] &lt;span style=&quot;color:#777&quot;&gt;# Pick the last columns that corresponds to the wining door&lt;/span&gt;

    &lt;span style=&quot;color:#777&quot;&gt;# First step : the player select a door randomly&lt;/span&gt;
    choices = np.random.randint(&lt;span style=&quot;color:#00D&quot;&gt;3&lt;/span&gt;, size=nb_exp)

    &lt;span style=&quot;color:#777&quot;&gt;# Second and third steps are now combined&lt;/span&gt;
    &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;if&lt;/span&gt; change_door:
        &lt;span style=&quot;color:#777&quot;&gt;# If change_door, means that the wining door is not the one originaly picked&lt;/span&gt;
        &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;return&lt;/span&gt; np.mean(car_doors!=choices)
    &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;return&lt;/span&gt; np.mean(car_doors==choices)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The result is still the same but we just improved our computation speed by a wooping factor of &lt;strong&gt;300+&lt;/strong&gt; !&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;%%time
np.mean([montyhall_sim(&lt;span style=&quot;color:#069&quot;&gt;False&lt;/span&gt;) &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;for&lt;/span&gt; x &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;in&lt;/span&gt; &lt;span style=&quot;color:#369;font-weight:bold&quot;&gt;range&lt;/span&gt;(&lt;span style=&quot;color:#00D&quot;&gt;1000000&lt;/span&gt;)])

&amp;gt; Wall time: &lt;span style=&quot;color:#60E&quot;&gt;90.8&lt;/span&gt; ms

&amp;gt; &lt;span style=&quot;color:#60E&quot;&gt;0.333379&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It&amp;#8217;s a good habit to look for algorithmic optimization when possible and I am sure that someone could still find ways to improve this one ;)&lt;/p&gt;
&lt;/div&gt;</content>

			
				<category term="probabilities" />
			
			
				<category term="probabilities" />
			

			<published>2020-08-07T00:00:00+00:00</published>
		</entry>
	
		<entry>
			<id>https://yobadia.com/reinforcement%20learning/finite_markov_process/</id>
			<title>RL: Markov Finite Decision Process</title>
			<link href="https://yobadia.com/reinforcement%20learning/finite_markov_process/" rel="alternate" type="text/html" title="RL: Markov Finite Decision Process" />
			<updated>2020-06-18T00:00:00+00:00</updated>

			
				
				<author>
					
						<name>yo</name>
					
					
					
				</author>
			
			<summary>Learn RL from the ground up</summary>
			<content type="html" xml:base="https://yobadia.com/reinforcement%20learning/finite_markov_process/">&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\DeclareMathOperator{\EX}{\mathbb{E}}% expected value\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Those notes are based on the Coursera Reinforcement Learning course and the book
&quot;Reinforcement Learning An Introduction Second Edition&quot; from Richard S. Sutton and Andrew G. Barto.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Based on the chapter 3 of the book.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;k-armed-bandit-problem&quot;&gt;k-armed Bandit Problem&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The purpose of this game is to maximize a total reward over some discreat time period with a stationnary probability distribution.
The most common example of this framework is a slot machine with \(k\) levers each one with an &lt;em&gt;expected reward&lt;/em&gt;
space with \(k\) possible actions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[q_*(a) = \EX[R_t|A_t=a]\\
\text{ }\\
\text{with:}\\

\begin{align*}
    &amp;amp;A_t    &amp;amp;&amp;amp;\text{the action taken at time-step }t\\
    &amp;amp;a      &amp;amp;&amp;amp;\text{the action chosen}\\
    &amp;amp;R_t    &amp;amp;&amp;amp;\text{the }{\bf observed}\text{ reward associated with action }A_t\\
    &amp;amp;q_*(a) &amp;amp;&amp;amp;\text{the }{\bf expected}\text{ reward associated with action }a\\
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;However \(q_*(a)\) is not known, hence we introduce \(Q_t(a) = \EX_t(a)\) as a proxy of \(q_*(a)\) at time \(t\).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;exploitation&quot;&gt;Exploitation:&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Greedy action&lt;/strong&gt;: pick action \(a\) associated with the highest \(Q_t\) : \(A_t = argmax_a(Q_t)\)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;exploration&quot;&gt;Exploration:&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Nongreedy action&lt;/strong&gt;: pick action \(a\) not associated with the highest \(Q_t\).&lt;br&gt;
Examples:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;\(\epsilon\_\text{greedy}\) which converges to \(q_*(a)\) in stationnary settings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use a &lt;strong&gt;greedy&lt;/strong&gt; setup with &lt;em&gt;optimistic initial values&lt;/em&gt; \(Q_1(a)\) for all a. The unexplored ones will end up with
higher observed reward and will be selected next and so on. It is only suited for stationary problems.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;computing-q_t&quot;&gt;Computing \(Q_t\)&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The purpose is to have a computing method that is constant in time and memory for updating \(Q_t\)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[NewEstimate = OldEstimate + Stepsize[Target - OldEstimate] \\
[Target - OldEstimate] = \text{estimation error}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;stationnary-problems&quot;&gt;Stationnary problems&lt;/h3&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
Q_{t+1} &amp;amp;= \frac{1}{n}\sum_{i=1}^{n}{R_i}\\
        &amp;amp;= Q_n + \frac{1}{n}[R_n - Q_n]
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As \(t\) increases the Stepsize \(\frac{1}{n}\) decreases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;non-stationnary-problems&quot;&gt;Non-stationnary problems&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The purpose is to give more weight to recent information. Here we show an exemple of what is
called an &lt;em&gt;exponential recency-weighted average&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
&amp;amp;Q_{t+1} &amp;amp;&amp;amp;= Q_n + \alpha[R_n - Q_n]\\
&amp;amp;        &amp;amp;&amp;amp;= (1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\alpha\in[0,1]\]
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;upper-confidence-bound-action-selection&quot;&gt;Upper-Confidence-Bound Action Selection&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;\(\epsilon\)-greedy action selection picks randomly among the non-greedy actions without exploiting the information
on their expected values and the confidence on those values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here we show a better action selection method that improve the chances of selecting actions that could be optimal
for a &lt;strong&gt;stationary&lt;/strong&gt; problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[A_t = argmax_a[Q_t(a)+c\sqrt{\frac{log(t)}{N_t(a)}}]\\
\\
N_t(a) = \text{the number of time that action }a\text{ has been selected}\\
c = \text{a coefficient to control the degree of exploration}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;gradient-bandit-algorithms&quot;&gt;Gradient Bandit Algorithms&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here the purpose is not anymore to estimate &lt;em&gt;action-values&lt;/em&gt; but rather to learn a &lt;em&gt;preference&lt;/em&gt; for each action
\(a\) noted \(H_t(a)\). The preference has no interpretation in terms of rewards. It can be viewed as a
probability which is determined according to a &lt;em&gt;soft-max distribution&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}} = \pi_t(a)\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;\(\pi_t(a)\) the probability of taking action \(a\) at time \(t\).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The learning algorithm is based on &lt;em&gt;stochastic gradient ascent&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
&amp;amp;H_{t+1}(A_t) &amp;amp;&amp;amp;= H_t(A_t) + \alpha(R_t - \bar{R}_t)(1-pi_t(A_t)) &amp;amp;&amp;amp;&amp;amp; \text{for } a=A_t \text{ and}\\
&amp;amp;H_{t+1}(a) &amp;amp;&amp;amp;= H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) &amp;amp;&amp;amp;&amp;amp; \text{for all } a \neq A_t
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
&amp;amp;\alpha &amp;gt; 0 &amp;amp;&amp;amp;\text{a step-size parameter}\\

&amp;amp;\bar{R}_t \in {\rm I\!R} &amp;amp;&amp;amp;\text{the average of all rewards up to time } t \text{ included. Can be a simple average}\\
&amp;amp; &amp;amp;&amp;amp; \text{or a more fancy method.}\\

&amp;amp;R_t - \bar{R}_t &amp;amp;&amp;amp;\text{measures whether the new reward is higher than what its baseline value is. If } \\
&amp;amp; &amp;amp;&amp;amp; R_t&amp;gt;\bar{R}_t \text{then the probability of selecting } A_t \text{increases.}\\

&amp;amp;1 - \pi_t(A_t) &amp;amp;&amp;amp;\text{It corresponds to the sum of all probabilities for } a \neq A_t \text{ The higher this value}\\
&amp;amp; &amp;amp;&amp;amp; \text{the more }H_{t+1} \text{will change.}

\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Up to know we have only looked at &lt;strong&gt;non-associative tasks&lt;/strong&gt;. Those are tasks that do not rely on a state of the world
to evaluate the best action. Imagine a slot machine that changes the expected reward of a given action by changing its color.
The best strategy would be to search for the best action-value for each color of the machine. This is called &lt;strong&gt;associative
search task&lt;/strong&gt; or &lt;strong&gt;contextual bandit&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content>

			
				<category term="reinforcement learning" />
			
			
				<category term="reinforcement learning" />
			

			<published>2020-06-18T00:00:00+00:00</published>
		</entry>
	
		<entry>
			<id>https://yobadia.com/reinforcement%20learning/multi-armed-bandit/</id>
			<title>RL: Multi-Armed Bandits</title>
			<link href="https://yobadia.com/reinforcement%20learning/multi-armed-bandit/" rel="alternate" type="text/html" title="RL: Multi-Armed Bandits" />
			<updated>2020-06-10T00:00:00+00:00</updated>

			
				
				<author>
					
						<name>yo</name>
					
					
					
				</author>
			
			<summary>Learn RL from the ground up</summary>
			<content type="html" xml:base="https://yobadia.com/reinforcement%20learning/multi-armed-bandit/">&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\DeclareMathOperator{\EX}{\mathbb{E}}% expected value\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Those notes are based on the Coursera Reinforcement Learning course and the book
&quot;Reinforcement Learning An Introduction Second Edition&quot; from Richard S. Sutton and Andrew G. Barto.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Based on the chapter 2 of the book.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;k-armed-bandit-problem&quot;&gt;k-armed Bandit Problem&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The purpose of this game is to maximize a total reward over some discreat time period with a stationnary probability distribution.
The most common example of this framework is a slot machine with \(k\) levers each one with an &lt;em&gt;expected reward&lt;/em&gt;
space with \(k\) possible actions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[q_*(a) = \EX[R_t|A_t=a]\\
\text{ }\\
\text{with:}\\

\begin{align*}
    &amp;amp;A_t    &amp;amp;&amp;amp;\text{the action taken at time-step }t\\
    &amp;amp;a      &amp;amp;&amp;amp;\text{the action chosen}\\
    &amp;amp;R_t    &amp;amp;&amp;amp;\text{the }{\bf observed}\text{ reward associated with action }A_t\\
    &amp;amp;q_*(a) &amp;amp;&amp;amp;\text{the }{\bf expected}\text{ reward associated with action }a\\
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;However \(q_*(a)\) is not known, hence we introduce \(Q_t(a) = \EX_t(a)\) as a proxy of \(q_*(a)\) at time \(t\).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;exploitation&quot;&gt;Exploitation:&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Greedy action&lt;/strong&gt;: pick action \(a\) associated with the highest \(Q_t\) : \(A_t = argmax_a(Q_t)\)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;exploration&quot;&gt;Exploration:&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Nongreedy action&lt;/strong&gt;: pick action \(a\) not associated with the highest \(Q_t\).&lt;br&gt;
Examples:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;\(\epsilon\_\text{greedy}\) which converges to \(q_*(a)\) in stationnary settings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use a &lt;strong&gt;greedy&lt;/strong&gt; setup with &lt;em&gt;optimistic initial values&lt;/em&gt; \(Q_1(a)\) for all a. The unexplored ones will end up with
higher observed reward and will be selected next and so on. It is only suited for stationary problems.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;computing-q_t&quot;&gt;Computing \(Q_t\)&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The purpose is to have a computing method that is constant in time and memory for updating \(Q_t\)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[NewEstimate = OldEstimate + Stepsize[Target - OldEstimate] \\
[Target - OldEstimate] = \text{estimation error}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;stationnary-problems&quot;&gt;Stationnary problems&lt;/h3&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
Q_{t+1} &amp;amp;= \frac{1}{n}\sum_{i=1}^{n}{R_i}\\
        &amp;amp;= Q_n + \frac{1}{n}[R_n - Q_n]
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As \(t\) increases the Stepsize \(\frac{1}{n}\) decreases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;non-stationnary-problems&quot;&gt;Non-stationnary problems&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The purpose is to give more weight to recent information. Here we show an exemple of what is
called an &lt;em&gt;exponential recency-weighted average&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
&amp;amp;Q_{t+1} &amp;amp;&amp;amp;= Q_n + \alpha[R_n - Q_n]\\
&amp;amp;        &amp;amp;&amp;amp;= (1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\alpha\in[0,1]\]
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;upper-confidence-bound-action-selection&quot;&gt;Upper-Confidence-Bound Action Selection&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;\(\epsilon\)-greedy action selection picks randomly among the non-greedy actions without exploiting the information
on their expected values and the confidence on those values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here we show a better action selection method that improve the chances of selecting actions that could be optimal
for a &lt;strong&gt;stationary&lt;/strong&gt; problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[A_t = argmax_a[Q_t(a)+c\sqrt{\frac{log(t)}{N_t(a)}}]\\
\\
N_t(a) = \text{the number of time that action }a\text{ has been selected}\\
c = \text{a coefficient to control the degree of exploration}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;gradient-bandit-algorithms&quot;&gt;Gradient Bandit Algorithms&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here the purpose is not anymore to estimate &lt;em&gt;action-values&lt;/em&gt; but rather to learn a &lt;em&gt;preference&lt;/em&gt; for each action
\(a\) noted \(H_t(a)\). The preference has no interpretation in terms of rewards. It can be viewed as a
probability which is determined according to a &lt;em&gt;soft-max distribution&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}} = \pi_t(a)\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;\(\pi_t(a)\) the probability of taking action \(a\) at time \(t\).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The learning algorithm is based on &lt;em&gt;stochastic gradient ascent&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
&amp;amp;H_{t+1}(A_t) &amp;amp;&amp;amp;= H_t(A_t) + \alpha(R_t - \bar{R}_t)(1-pi_t(A_t)) &amp;amp;&amp;amp;&amp;amp; \text{for } a=A_t \text{ and}\\
&amp;amp;H_{t+1}(a) &amp;amp;&amp;amp;= H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) &amp;amp;&amp;amp;&amp;amp; \text{for all } a \neq A_t
\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;stemblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
\[\begin{align*}
&amp;amp;\alpha &amp;gt; 0 &amp;amp;&amp;amp;\text{a step-size parameter}\\

&amp;amp;\bar{R}_t \in {\rm I\!R} &amp;amp;&amp;amp;\text{the average of all rewards up to time } t \text{ included. Can be a simple average}\\
&amp;amp; &amp;amp;&amp;amp; \text{or a more fancy method.}\\

&amp;amp;R_t - \bar{R}_t &amp;amp;&amp;amp;\text{measures whether the new reward is higher than what its baseline value is. If } \\
&amp;amp; &amp;amp;&amp;amp; R_t&amp;gt;\bar{R}_t \text{then the probability of selecting } A_t \text{increases.}\\

&amp;amp;1 - \pi_t(A_t) &amp;amp;&amp;amp;\text{It corresponds to the sum of all probabilities for } a \neq A_t \text{ The higher this value}\\
&amp;amp; &amp;amp;&amp;amp; \text{the more }H_{t+1} \text{will change.}

\end{align*}\]
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Up to know we have only looked at &lt;strong&gt;non-associative tasks&lt;/strong&gt;. Those are tasks that do not rely on a state of the world
to evaluate the best action. Imagine a slot machine that changes the expected reward of a given action by changing its color.
The best strategy would be to search for the best action-value for each color of the machine. This is called &lt;strong&gt;associative
search task&lt;/strong&gt; or &lt;strong&gt;contextual bandit&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content>

			
				<category term="reinforcement learning" />
			
			
				<category term="reinforcement learning" />
			

			<published>2020-06-10T00:00:00+00:00</published>
		</entry>
	
		<entry>
			<id>https://yobadia.com/visualization/background_img/</id>
			<title>How to create that blog background image</title>
			<link href="https://yobadia.com/visualization/background_img/" rel="alternate" type="text/html" title="How to create that blog background image" />
			<updated>2020-04-24T00:00:00+00:00</updated>

			
				
				<author>
					
						<name>yo</name>
					
					
					
				</author>
			
			<summary>Use matplotlib for creativity !</summary>
			<content type="html" xml:base="https://yobadia.com/visualization/background_img/">&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This blog post is meant for me to experiment with the Asciidoc syntax to
move away from Markdown and integrate it to this jekyll template, as
well as to document how I produced my background image.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;preparing-the-environment&quot;&gt;Preparing the environment&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#777&quot;&gt;# Import relevant libraries&lt;/span&gt;
&lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;import&lt;/span&gt; &lt;span style=&quot;color:#B44;font-weight:bold&quot;&gt;numpy&lt;/span&gt; &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;as&lt;/span&gt; np
&lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;import&lt;/span&gt; &lt;span style=&quot;color:#B44;font-weight:bold&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span style=&quot;color:#080;font-weight:bold&quot;&gt;as&lt;/span&gt; plt

&lt;span style=&quot;color:#777&quot;&gt;# Define the figsize for all pyplot plots&lt;/span&gt;
plt.rcParams[&lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#710&quot;&gt;'&lt;/span&gt;&lt;span style=&quot;color:#D20&quot;&gt;figure.figsize&lt;/span&gt;&lt;span style=&quot;color:#710&quot;&gt;'&lt;/span&gt;&lt;/span&gt;] = &lt;span style=&quot;color:#00D&quot;&gt;10&lt;/span&gt;, &lt;span style=&quot;color:#00D&quot;&gt;15&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;define-useful-parameters&quot;&gt;Define useful parameters&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;size = &lt;span style=&quot;color:#00D&quot;&gt;500&lt;/span&gt;
loc = &lt;span style=&quot;color:#00D&quot;&gt;5&lt;/span&gt;
scale = &lt;span style=&quot;color:#60E&quot;&gt;1.8&lt;/span&gt;
x_factor = &lt;span style=&quot;color:#60E&quot;&gt;0.5&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;generate-the-values-to-be-ploted&quot;&gt;Generate the values to be ploted&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here we use gaussian distributions for generatin the image but you could use any to generate an image that suits you better.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#777&quot;&gt;# Create 2 gaussian blobs distributed around 0 and concatenate them in a single vector&lt;/span&gt;
x = np.random.normal(size=size, loc=loc, scale=scale)
x = np.concatenate([x, -x])

&lt;span style=&quot;color:#777&quot;&gt;# Create another vector as a combination of the previous one and a new gausian blob&lt;/span&gt;
y = (x * x_factor + np.random.normal(size=size*&lt;span style=&quot;color:#00D&quot;&gt;2&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;create-and-save-the-plot&quot;&gt;Create and save the plot&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here the colormap used is 'Reds_r' and for the real background image it was 'Blues_r'.
To find a list of all available colors you can follow this &lt;a href=&quot;https://matplotlib.org/tutorials/colors/colormaps.html&quot; target=&quot;\&quot;_blank\&quot;&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;python&quot;&gt;plt.hexbin(x, y, gridsize=(&lt;span style=&quot;color:#00D&quot;&gt;20&lt;/span&gt;,&lt;span style=&quot;color:#00D&quot;&gt;20&lt;/span&gt;), cmap=plt.get_cmap(&lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#710&quot;&gt;'&lt;/span&gt;&lt;span style=&quot;color:#D20&quot;&gt;Reds_r&lt;/span&gt;&lt;span style=&quot;color:#710&quot;&gt;'&lt;/span&gt;&lt;/span&gt;))
plt.savefig(&lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#710&quot;&gt;'&lt;/span&gt;&lt;span style=&quot;color:#D20&quot;&gt;background_red.png&lt;/span&gt;&lt;span style=&quot;color:#710&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, dpi=&lt;span style=&quot;color:#00D&quot;&gt;500&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;../../images/background_red.png&quot; alt=&quot;background red&quot;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once this image is generated, you can use your favorite image editor to
rotate it, crop it, etc… Be creative :)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content>

			
				<category term="visualization" />
			
			
				<category term="data-viz" />
			
				<category term="python" />
			
				<category term="art" />
			

			<published>2020-04-24T00:00:00+00:00</published>
		</entry>
	
</feed>