<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>RL: Multi-Armed Bandits</title><link rel="stylesheet" type="text/css" href="https://yobadia.com/assets/css/styles_feeling_responsive.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.0/css/font-awesome.min.css"> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://yobadia.com/assets/js/modernizr.min.js"></script> <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script> <script> WebFont.load({ google: { families: [ 'Lato:400,700,400italic:latin', 'Volkhov::latin' ] } }); </script> <noscript><link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic%7CVolkhov' rel='stylesheet' type='text/css'> </noscript> <script> MathJax.Hub.Config({ jax: ["input/TeX","output/HTML-CSS"], displayAlign: "left" }); </script><meta name="description" content="Learn RL from the ground up"><meta name="google-site-verification" content="Vk0IOJ2jwG_qEoG7fuEXYqv0m2rLa8P778Fi_GrsgEQ"><meta name="msvalidate.01" content="0FB4C028ABCF07C908C54386ABD2D97F" ><link rel="author" href="https://plus.google.com/u/0/118311555303973066167"><link rel="canonical" href="https://yobadia.com/reinforcement%20learning/multi-armed-bandit/"><meta property="og:title" content="RL: Multi-Armed Bandits"><meta property="og:description" content="Learn RL from the ground up"><meta property="og:url" content="https://yobadia.com/reinforcement%20learning/multi-armed-bandit/"><meta property="og:locale" content="en_EN"><meta property="og:type" content="website"><meta property="og:site_name" content="Data thoughts"><meta property="og:image" content="../../images/background_blue.png"><link type="text/plain" rel="author" href="https://yobadia.com/humans.txt"><link rel="icon" sizes="32x32" href="https://yobadia.com/assets/img/favicon-32x32.png"><link rel="icon" sizes="192x192" href="https://yobadia.com/assets/img/touch-icon-192x192.png"><link rel="apple-touch-icon-precomposed" sizes="180x180" href="https://yobadia.com/assets/img/apple-touch-icon-180x180-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="152x152" href="https://yobadia.com/assets/img/apple-touch-icon-152x152-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://yobadia.com/assets/img/apple-touch-icon-144x144-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="120x120" href="https://yobadia.com/assets/img/apple-touch-icon-120x120-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://yobadia.com/assets/img/apple-touch-icon-114x114-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="76x76" href="https://yobadia.com/assets/img/apple-touch-icon-76x76-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://yobadia.com/assets/img/apple-touch-icon-72x72-precomposed.png"><link rel="apple-touch-icon-precomposed" href="https://yobadia.com/assets/img/apple-touch-icon-precomposed.png"><meta name="msapplication-TileImage" content="https://yobadia.com/assets/img/msapplication_tileimage.png"><meta name="msapplication-TileColor" content="#fabb00"><body id="top-of-page" class="post"><div id="navigation" class="sticky"><nav class="top-bar" role="navigation" data-topbar><ul class="title-area">
<li class="name"><h1 class="show-for-small-only"><a href="https://yobadia.com" class="icon-tree"> Data thoughts</a></h1></li>
<li class="toggle-topbar menu-icon"><a href="#"><span>Nav</span></a></li>
</ul>
<section class="top-bar-section"><ul class="right">
<li class="divider">
<li><a href="https://yobadia.com/info/">About</a></li>
</ul>
<ul class="left">
<li><a href="https://yobadia.com/">Home</a></li>
<li class="divider">
<li><a href="https://yobadia.com/blog/">Blog</a></li>
<li class="divider">
</ul></section></nav></div><div class="row t30"><div class="medium-8 columns medium-offset-2 end"><article itemscope itemtype="http://schema.org/Article"><header><figure> <img src="../../images/background_blue.png" width="970" alt="RL: Multi-Armed Bandits" itemprop="image"></figure><div itemprop="name">
<p class="subheadline">Book notes</p>
<h1>RL: Multi-Armed Bandits</h1>
</div></header><p class="teaser" itemprop="description"> Learn RL from the ground up</p>
<div itemprop="articleSection">
<div class="stemblock"><div class="content"> \[\DeclareMathOperator{\EX}{\mathbb{E}}% expected value\]</div></div>
<div class="paragraph"><p>Those notes are based on the Coursera Reinforcement Learning course and the book "Reinforcement Learning An Introduction Second Edition" from Richard S. Sutton and Andrew G. Barto.</p></div>
<div class="paragraph"><p>Based on the chapter 2 of the book.</p></div>
<div class="sect1">
<h2 id="k-armed-bandit-problem">k-armed Bandit Problem</h2>
<div class="sectionbody">
<div class="paragraph"><p>The purpose of this game is to maximize a total reward over some discreat time period with a stationnary probability distribution. The most common example of this framework is a slot machine with \(k\) levers each one with an <em>expected reward</em> space with \(k\) possible actions.</p></div>
<div class="stemblock"><div class="content"> \[q_*(a) = \EX[R_t|A_t=a]\\ \text{ }\\ \text{with:}\\ \begin{align*} &amp;A_t &amp;&amp;\text{the action taken at time-step }t\\ &amp;a &amp;&amp;\text{the action chosen}\\ &amp;R_t &amp;&amp;\text{the }{\bf observed}\text{ reward associated with action }A_t\\ &amp;q_*(a) &amp;&amp;\text{the }{\bf expected}\text{ reward associated with action }a\\ \end{align*}\]</div></div>
<div class="paragraph"><p>However \(q_*(a)\) is not known, hence we introduce \(Q_t(a) = \EX_t(a)\) as a proxy of \(q_*(a)\) at time \(t\).</p></div>
<div class="sect2">
<h3 id="exploitation">Exploitation:</h3>
<div class="paragraph"><p><strong>Greedy action</strong>: pick action \(a\) associated with the highest \(Q_t\) : \(A_t = argmax_a(Q_t)\)</p></div>
</div>
<div class="sect2">
<h3 id="exploration">Exploration:</h3>
<div class="paragraph"><p><strong>Nongreedy action</strong>: pick action \(a\) not associated with the highest \(Q_t\).<br> Examples:</p></div>
<div class="ulist"><ul>
<li><p>\(\epsilon\_\text{greedy}\) which converges to \(q_*(a)\) in stationnary settings.</p></li>
<li><p>Use a <strong>greedy</strong> setup with <em>optimistic initial values</em> \(Q_1(a)\) for all a. The unexplored ones will end up with higher observed reward and will be selected next and so on. It is only suited for stationary problems.</p></li>
</ul></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="computing-q_t">Computing \(Q_t\)</h2>
<div class="sectionbody">
<div class="paragraph"><p>The purpose is to have a computing method that is constant in time and memory for updating \(Q_t\)</p></div>
<div class="stemblock"><div class="content"> \[NewEstimate = OldEstimate + Stepsize[Target - OldEstimate] \\ [Target - OldEstimate] = \text{estimation error}\]</div></div>
<div class="sect2">
<h3 id="stationnary-problems">Stationnary problems</h3>
<div class="stemblock"><div class="content"> \[\begin{align*} Q_{t+1} &amp;= \frac{1}{n}\sum_{i=1}^{n}{R_i}\\ &amp;= Q_n + \frac{1}{n}[R_n - Q_n] \end{align*}\]</div></div>
<div class="paragraph"><p>As \(t\) increases the Stepsize \(\frac{1}{n}\) decreases.</p></div>
</div>
<div class="sect2">
<h3 id="non-stationnary-problems">Non-stationnary problems</h3>
<div class="paragraph"><p>The purpose is to give more weight to recent information. Here we show an exemple of what is called an <em>exponential recency-weighted average</em></p></div>
<div class="stemblock"><div class="content"> \[\begin{align*} &amp;Q_{t+1} &amp;&amp;= Q_n + \alpha[R_n - Q_n]\\ &amp; &amp;&amp;= (1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i \end{align*}\]</div></div>
<div class="stemblock"><div class="content"> \[\alpha\in[0,1]\]</div></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="upper-confidence-bound-action-selection">Upper-Confidence-Bound Action Selection</h2>
<div class="sectionbody">
<div class="paragraph"><p>\(\epsilon\)-greedy action selection picks randomly among the non-greedy actions without exploiting the information on their expected values and the confidence on those values.</p></div>
<div class="paragraph"><p>Here we show a better action selection method that improve the chances of selecting actions that could be optimal for a <strong>stationary</strong> problem.</p></div>
<div class="stemblock"><div class="content"> \[A_t = argmax_a[Q_t(a)+c\sqrt{\frac{log(t)}{N_t(a)}}]\\ \\ N_t(a) = \text{the number of time that action }a\text{ has been selected}\\ c = \text{a coefficient to control the degree of exploration}\]</div></div>
</div>
</div>
<div class="sect1">
<h2 id="gradient-bandit-algorithms">Gradient Bandit Algorithms</h2>
<div class="sectionbody">
<div class="paragraph"><p>Here the purpose is not anymore to estimate <em>action-values</em> but rather to learn a <em>preference</em> for each action \(a\) noted \(H_t(a)\). The preference has no interpretation in terms of rewards. It can be viewed as a probability which is determined according to a <em>soft-max distribution</em>.</p></div>
<div class="stemblock"><div class="content"> \[Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}} = \pi_t(a)\]</div></div>
<div class="paragraph"><p>\(\pi_t(a)\) the probability of taking action \(a\) at time \(t\).</p></div>
<div class="paragraph"><p>The learning algorithm is based on <em>stochastic gradient ascent</em>:</p></div>
<div class="stemblock"><div class="content"> \[\begin{align*} &amp;H_{t+1}(A_t) &amp;&amp;= H_t(A_t) + \alpha(R_t - \bar{R}_t)(1-pi_t(A_t)) &amp;&amp;&amp; \text{for } a=A_t \text{ and}\\ &amp;H_{t+1}(a) &amp;&amp;= H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) &amp;&amp;&amp; \text{for all } a \neq A_t \end{align*}\]</div></div>
<div class="stemblock"><div class="content"> \[\begin{align*} &amp;\alpha &gt; 0 &amp;&amp;\text{a step-size parameter}\\ &amp;\bar{R}_t \in {\rm I\!R} &amp;&amp;\text{the average of all rewards up to time } t \text{ included. Can be a simple average}\\ &amp; &amp;&amp; \text{or a more fancy method.}\\ &amp;R_t - \bar{R}_t &amp;&amp;\text{measures whether the new reward is higher than what its baseline value is. If } \\ &amp; &amp;&amp; R_t&gt;\bar{R}_t \text{then the probability of selecting } A_t \text{increases.}\\ &amp;1 - \pi_t(A_t) &amp;&amp;\text{It corresponds to the sum of all probabilities for } a \neq A_t \text{ The higher this value}\\ &amp; &amp;&amp; \text{the more }H_{t+1} \text{will change.} \end{align*}\]</div></div>
<div class="paragraph"><p>Up to know we have only looked at <strong>non-associative tasks</strong>. Those are tasks that do not rely on a state of the world to evaluate the best action. Imagine a slot machine that changes the expected reward of a given action by changing its color. The best strategy would be to search for the best action-value for each color of the machine. This is called <strong>associative search task</strong> or <strong>contextual bandit</strong>.</p></div>
</div>
</div>
</div>
<h3 id="comments" class="t60">Dialogue &amp; Discussion</h3>
<div id="disqus_thread"></div>
<script type="text/javascript"> /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */ var disqus_shortname = 'yobadia'; var disqus_identifier = '/reinforcement%20learning/multi-armed-bandit/'; /* * * DON'T EDIT BELOW THIS LINE * * */ (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a>
</noscript></article></div></div><div id="up-to-top" class="row"><div class="small-12 columns" style="text-align: right;"> <a class="iconfont" href="#top-of-page"></a>
</div></div><div id="subfooter"><nav class="row"><section id="subfooter-left" class="small-12 medium-6 columns credits"><p><a href="http://phlow.github.io/feeling-responsive/" target="_blank" rel="noopener noreferrer">Feeling Responsive</a> by <a href="http://phlow.de/" target="_blank" rel="noopener noreferrer">Phlow</a> with ♥</p></section><section id="subfooter-right" class="small-12 medium-6 columns"><ul class="inline-list social-icons">
<li><a href="https://github.com/YohanObadia/" target="_blank" class="icon-github" title="Code und mehr..." rel="noopener noreferrer"></a></li>
<li><a href="https://www.linkedin.com/in/yohan-obadia/" target="_blank" class="icon-linkedin" title="YouTube Google+" rel="noopener noreferrer"></a></li>
</ul></section></nav></div><script src="https://yobadia.com/assets/js/javascript.min.js"></script>
