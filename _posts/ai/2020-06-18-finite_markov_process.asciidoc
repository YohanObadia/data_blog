---
layout: page

subheadline: "Book notes"
title: "RL: Markov Finite Decision Process"
teaser: "Learn RL from the ground up"
categories:
  - reinforcement learning
tags:
  - reinforcement learning

header: no

comments: true
show_meta: false

image:
    title: ../../images/background_blue.png
    thumb: ../../images/background_blue.png
    homepage: ../../images/background_blue.png
author: yo

---
include::_posts/math.adoc[leveloffset=+1]

Those notes are based on the Coursera Reinforcement Learning course and the book 
"Reinforcement Learning An Introduction Second Edition" from Richard S. Sutton and Andrew G. Barto.

Based on the chapter 3 of the book.

== k-armed Bandit Problem

The purpose of this game is to maximize a total reward over some discreat time period with a stationnary probability distribution.
The most common example of this framework is a slot machine with stem:[k] levers each one with an __expected reward__
space with stem:[k] possible actions.

[stem, align=left] 
++++
q_*(a) = \EX[R_t|A_t=a]\\
\text{ }\\
\text{with:}\\

\begin{align*}
    &A_t    &&\text{the action taken at time-step }t\\
    &a      &&\text{the action chosen}\\
    &R_t    &&\text{the }{\bf observed}\text{ reward associated with action }A_t\\
    &q_*(a) &&\text{the }{\bf expected}\text{ reward associated with action }a\\
\end{align*}
++++

However stem:[q_*(a)] is not known, hence we introduce stem:[Q_t(a) = \EX_t(a)] as a proxy of stem:[q_*(a)] at time stem:[t].

=== Exploitation:
**Greedy action**: pick action stem:[a] associated with the highest stem:[Q_t] : stem:[A_t = argmax_a(Q_t)]

=== Exploration:
**Nongreedy action**: pick action stem:[a] not associated with the highest stem:[Q_t]. +
Examples: 

- stem:[\epsilon\_\text{greedy}] which converges to stem:[q_*(a)] in stationnary settings.
- Use a *greedy* setup with __optimistic initial values__ stem:[Q_1(a)] for all a. The unexplored ones will end up with 
higher observed reward and will be selected next and so on. It is only suited for stationary problems.

== Computing stem:[Q_t]

The purpose is to have a computing method that is constant in time and memory for updating stem:[Q_t]

[stem, align=left]
++++
NewEstimate = OldEstimate + Stepsize[Target - OldEstimate] \\
[Target - OldEstimate] = \text{estimation error}
++++

=== Stationnary problems

[stem, align=left] 
++++
\begin{align*}
Q_{t+1} &= \frac{1}{n}\sum_{i=1}^{n}{R_i}\\
        &= Q_n + \frac{1}{n}[R_n - Q_n]
\end{align*}
++++
As stem:[t] increases the Stepsize stem:[\frac{1}{n}] decreases.

=== Non-stationnary problems

The purpose is to give more weight to recent information. Here we show an exemple of what is 
called an __exponential recency-weighted average__

[stem, align=left] 
++++
\begin{align*}
&Q_{t+1} &&= Q_n + \alpha[R_n - Q_n]\\
&        &&= (1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
\end{align*}
++++

[stem, align=left] 
++++
\alpha\in[0,1]
++++

== Upper-Confidence-Bound Action Selection
stem:[\epsilon]-greedy action selection picks randomly among the non-greedy actions without exploiting the information 
on their expected values and the confidence on those values.

Here we show a better action selection method that improve the chances of selecting actions that could be optimal 
for a *stationary* problem.

[stem, align=left]
++++
A_t = argmax_a[Q_t(a)+c\sqrt{\frac{log(t)}{N_t(a)}}]\\
\\
N_t(a) = \text{the number of time that action }a\text{ has been selected}\\
c = \text{a coefficient to control the degree of exploration}
++++

== Gradient Bandit Algorithms

Here the purpose is not anymore to estimate __action-values__ but rather to learn a __preference__ for each action 
stem:[a] noted stem:[H_t(a)]. The preference has no interpretation in terms of rewards. It can be viewed as a 
probability which is determined according to a __soft-max distribution__.

[stem, align=left]
++++
Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}} = \pi_t(a)
++++

stem:[\pi_t(a)] the probability of taking action stem:[a] at time stem:[t].

The learning algorithm is based on __stochastic gradient ascent__:

[stem, align=left]
++++
\begin{align*}
&H_{t+1}(A_t) &&= H_t(A_t) + \alpha(R_t - \bar{R}_t)(1-pi_t(A_t)) &&& \text{for } a=A_t \text{ and}\\
&H_{t+1}(a) &&= H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) &&& \text{for all } a \neq A_t
\end{align*}
++++

[stem, align=left]
++++
\begin{align*}
&\alpha > 0 &&\text{a step-size parameter}\\

&\bar{R}_t \in {\rm I\!R} &&\text{the average of all rewards up to time } t \text{ included. Can be a simple average}\\
& && \text{or a more fancy method.}\\

&R_t - \bar{R}_t &&\text{measures whether the new reward is higher than what its baseline value is. If } \\
& && R_t>\bar{R}_t \text{then the probability of selecting } A_t \text{increases.}\\

&1 - \pi_t(A_t) &&\text{It corresponds to the sum of all probabilities for } a \neq A_t \text{ The higher this value}\\ 
& && \text{the more }H_{t+1} \text{will change.}

\end{align*}
++++

Up to know we have only looked at *non-associative tasks*. Those are tasks that do not rely on a state of the world
to evaluate the best action. Imagine a slot machine that changes the expected reward of a given action by changing its color.
The best strategy would be to search for the best action-value for each color of the machine. This is called *associative 
search task* or *contextual bandit*.